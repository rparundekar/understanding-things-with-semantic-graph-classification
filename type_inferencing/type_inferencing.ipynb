{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input, Dense, Activation,Dropout\n",
    "from keras import regularizers\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    -----------------Inputs-------------------------\n",
    "    These are the inputs for the run. \n",
    "    datasetFolder -  Change to the dataset folder generated by InMemoryGraphLoader.java.\n",
    "    numberOfFiles - Set to the number of files in the dataset.\n",
    "'''\n",
    "#Folder for the dataset\n",
    "#\n",
    "datasetFolder = '/home/carnd/dbpedia2016/all4_2x125/dataset/'\n",
    "\n",
    "#Number of files\n",
    "numberOfFiles = 638\n",
    "\n",
    "#Test split\n",
    "testSplit=0.1\n",
    "validationSplit=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_data(datasetFolder, datasetXFile, datasetYFile, wrap=True, printIt=False):\n",
    "    '''\n",
    "        Function to load the data given one file. \n",
    "        While the extension is .csv, we actually have a sparse representation internally.\n",
    "        This function unrolls the sparse representation into a feature and target vector numpy arrays\n",
    "        by using an encoding strategy if needed.\n",
    "        - datasetFolder: The dataset folder\n",
    "        - datasetXFile: The name of one batch file with feature data\n",
    "        - datasetYFile: The name of the file with target data for same batch\n",
    "        - wrap: True if the data should be incoded in feature vector of length 8384\n",
    "               False otherwise       \n",
    "    '''\n",
    "    # find number of rows and columns to create the numpy array for feature vector\n",
    "    with open(datasetFolder + datasetXFile, \"r\") as f:\n",
    "        head = f.readline()\n",
    "        cols = head.split(',')\n",
    "        numberOfCols = len(cols)\n",
    "        #print(numberOfCols)\n",
    "        numberOfRows=0\n",
    "        for line in f:\n",
    "            numberOfRows+=1\n",
    "        f.close()\n",
    "    if(printIt):\n",
    "        print('Input Features: {} x {}'.format(numberOfRows,numberOfCols-1))\n",
    "        \n",
    "    #Max width of feature vector for encoding\n",
    "    if(wrap==True):\n",
    "        maxY = 8384\n",
    "    else:\n",
    "        maxY = numberOfCols-1\n",
    "        \n",
    "    #Scaling factor (shift, actually)\n",
    "    half=(numberOfCols//maxY)*0.5\n",
    "    \n",
    "    # Initialize the numpy array\n",
    "    dataX = np.zeros([numberOfRows,maxY],np.int8)\n",
    "    with open(datasetFolder + datasetXFile, \"r\") as f:\n",
    "        head = f.readline() #Skip the header\n",
    "        rowCounter=0\n",
    "        for line in f:\n",
    "            # Read each line\n",
    "            row=line.split(',')\n",
    "            for i in range(1, len(row)):\n",
    "                if(int(row[i])<=0):\n",
    "                    continue;\n",
    "                #Create the value based on the encoding\n",
    "                val = 1 + ((int(row[i])-1)//maxY);\n",
    "                if(val>half):\n",
    "                    val = 0 - (val - half) #Scale if needed\n",
    "                #Assign the value\n",
    "                dataX[rowCounter][(int(row[i])-1)%maxY]= val\n",
    "            rowCounter+=1\n",
    "        f.close()\n",
    "   \n",
    "    # Read target vector file to find the size\n",
    "    with open(datasetFolder + datasetYFile, \"r\") as f:\n",
    "        head = f.readline()\n",
    "        cols = head.split(',')\n",
    "        numberOfCols = len(cols)\n",
    "        numberOfRows=0\n",
    "        for line in f:\n",
    "            numberOfRows+=1\n",
    "        f.close()\n",
    "\n",
    "    if(printIt):\n",
    "        print('Output Features: {} x {}'.format(numberOfRows,numberOfCols-1))\n",
    "\n",
    "    #Create the target vector numpy array\n",
    "    dataY = np.zeros([numberOfRows,(numberOfCols-1)],np.float16)\n",
    "    with open(datasetFolder + datasetYFile, \"r\") as f:\n",
    "        head = f.readline() #Skip the header\n",
    "        rowCounter=0\n",
    "        #Read each line and set the target class value to 1\n",
    "        for line in f:\n",
    "            row=line.split(',')\n",
    "            for i in range(1, len(row)):\n",
    "                if(int(row[i])<=0):\n",
    "                    continue;\n",
    "                dataY[rowCounter][(int(row[i])-1)]=1\n",
    "            rowCounter+=1\n",
    "        f.close()\n",
    "        \n",
    "    #Return\n",
    "    return dataX, dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Features: 4995 x 702241\n",
      "Output Features: 4995 x 525\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "dataX, dataY = load_data(datasetFolder,'datasetX_1.csv', 'datasetY_1.csv', printIt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Test load without print\n",
    "dataX, dataY = load_data(datasetFolder,'datasetX_1.csv', 'datasetY_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4995, 8384)\n",
      "[[1 1 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " [1 1 1 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#Details of features\n",
    "print(dataX.shape)\n",
    "print(dataX[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4995, 525)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#Details of targets\n",
    "print(dataY.shape)\n",
    "print(dataY[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Features for classification: 8384\n",
      "Output Classes for classification: 525\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Features for classification: {}\".format(dataX.shape[1]))\n",
    "print(\"Output Classes for classification: {}\".format(dataY.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create the final model after all the refinements.\n",
    "deepModel = Sequential(name='Deep Model (5 Dense Layers)')\n",
    "deepModel.add(Dense(2048, input_dim=dataX.shape[1], init='glorot_normal'))\n",
    "deepModel.add(BatchNormalization())\n",
    "deepModel.add(Activation('relu'))\n",
    "deepModel.add(Dropout(0.2))\n",
    "deepModel.add(Dense(1024, init='glorot_normal'))\n",
    "deepModel.add(BatchNormalization())\n",
    "deepModel.add(Activation('relu'))\n",
    "deepModel.add(Dropout(0.2))\n",
    "deepModel.add(Dense(768, init='glorot_normal'))\n",
    "deepModel.add(BatchNormalization())\n",
    "deepModel.add(Activation('relu'))\n",
    "deepModel.add(Dropout(0.2))\n",
    "deepModel.add(Dense(512, init='glorot_normal'))\n",
    "deepModel.add(BatchNormalization())\n",
    "deepModel.add(Activation('relu'))\n",
    "deepModel.add(Dropout(0.2))\n",
    "deepModel.add(Dense(256, init='glorot_normal'))\n",
    "deepModel.add(BatchNormalization())\n",
    "deepModel.add(Activation('relu'))\n",
    "deepModel.add(Dropout(0.2))\n",
    "deepModel.add(Dense(dataY.shape[1], activation='sigmoid', init='glorot_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "import keras.backend as K\n",
    "\n",
    "deepModel.compile(loss='binary_crossentropy', optimizer='nadam', metrics=[f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define important functions for calculating F1-score\n",
    "#(Based on Keras code, which is no-longer part of the codebase)\n",
    "def count_predictions(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives, predicted_positives, possible_positives\n",
    "\n",
    "def f1score(y_true, y_pred):\n",
    "    true_positives, predicted_positives, possible_positives = count_predictions(y_true, y_pred)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1score = 2.0 * precision * recall / (precision+recall+ K.epsilon())\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Helper function for counting\n",
    "def countPredictions(y_true, y_pred):\n",
    "    true_positives = np.sum(np.round(y_pred*y_true))\n",
    "    predicted_positives = np.sum(np.round(y_pred))\n",
    "    possible_positives = np.sum(y_true)\n",
    "    return true_positives, predicted_positives, possible_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Eon 1/8\n",
      " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9112\t precision = 0.9350 \t recall = 0.8886\n",
      "2. Eon 2/8\n",
      " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9174\t precision = 0.9359 \t recall = 0.8997\n",
      "3. Eon 3/8\n",
      " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9195\t precision = 0.9365 \t recall = 0.9032\n",
      "4. Eon 4/8\n",
      " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9194\t precision = 0.9340 \t recall = 0.9054\n",
      "5. Eon 5/8\n",
      " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9205\t precision = 0.9368 \t recall = 0.9048\n",
      "6. Eon 6/8\n",
      " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9202\t precision = 0.9359 \t recall = 0.9051\n",
      "7. Eon 7/8\n",
      " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9203\t precision = 0.9351 \t recall = 0.9061\n",
      "8. Eon 8/8\n",
      " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9199\t precision = 0.9350 \t recall = 0.9053\n",
      " - Final Test Score for Deep Model (5 Dense Layers) \t f1-score = 0.9200\t precision = 0.9352 \t recall = 0.9054\n"
     ]
    }
   ],
   "source": [
    "#Randomize the list of numbers so we can split train and test dataset\n",
    "listOfFiles=list(range(1,numberOfFiles+1))\n",
    "random.shuffle(listOfFiles) #Randomize the file order\n",
    "#Split index separates training and validataion data\n",
    "splitIndex=int((1-(testSplit+validationSplit))*numberOfFiles) \n",
    "#Test split index separates validation and test data\n",
    "testSplitIndex=int((1-(testSplit))*numberOfFiles)\n",
    "\n",
    "#Eons are similar to epochs, except since the model.fit function also uses epoch, \n",
    "#we switch to a name with longer time horizon\n",
    "numberOfEons = 8\n",
    "for eon in range(0, numberOfEons):\n",
    "    print('{}. Eon {}/{}'.format(eon+1,eon+1, numberOfEons))\n",
    "    #Train the data\n",
    "    for trainIndex in range(0,splitIndex):\n",
    "        dataX, dataY = load_data(datasetFolder,'datasetX_{}.csv'.format(listOfFiles[trainIndex]), 'datasetY_{}.csv'.format(listOfFiles[trainIndex]))\n",
    "        deepModel.fit(dataX, dataY, nb_epoch=1, verbose=0, batch_size=256)\n",
    "        print('Learning deep model for file {} / {} : datasetX/Y_{}'.format(trainIndex+1, splitIndex, listOfFiles[trainIndex]), end='\\r')\n",
    "\n",
    "    #Hold the counts of the predictions for the validation\n",
    "    counts = {} \n",
    "    counts[deepModel.name] = {'true_positives':0, 'predicted_positives':0, 'possible_positives':0}\n",
    "    \n",
    "    #Validation\n",
    "    for testIndex in range(splitIndex, testSplitIndex):\n",
    "        dataX, dataY = load_data(datasetFolder,'datasetX_{}.csv'.format(listOfFiles[testIndex]), 'datasetY_{}.csv'.format(listOfFiles[testIndex]))\n",
    "        predY=deepModel.predict_on_batch(dataX)\n",
    "        true_positives, predicted_positives, possible_positives = countPredictions(dataY, predY)\n",
    "        counts[deepModel.name]['true_positives'] += true_positives\n",
    "        counts[deepModel.name]['predicted_positives'] += predicted_positives\n",
    "        counts[deepModel.name]['possible_positives'] += possible_positives\n",
    "        print ('Validating deep model {} / {} : - true +ve:{}  pred +ve:{} possible +ve:{}'.format(testIndex+1, testSplitIndex, true_positives,predicted_positives,possible_positives), end='\\r')\n",
    "    \n",
    "    #Metrics calculation\n",
    "    count = counts[deepModel.name]\n",
    "    precision = (count['true_positives'])/(count['predicted_positives']+0.0001)\n",
    "    recall = (count['true_positives'])/(count['possible_positives']+0.0001)\n",
    "    f1score = 2.0 * precision * recall / (precision+recall+0.0001)\n",
    "    print(' - Model = {} \\t f1-score = {:.4f}\\t precision = {:.4f} \\t recall = {:.4f}'.format(deepModel.name, f1score, precision, recall))\n",
    "\n",
    "#Reinitialize counters\n",
    "counts = {} \n",
    "counts[deepModel.name] = {'true_positives':0, 'predicted_positives':0, 'possible_positives':0}\n",
    "#Testing \n",
    "for testIndex in range(testSplitIndex, numberOfFiles):\n",
    "    dataX, dataY = load_data(datasetFolder,'datasetX_{}.csv'.format(listOfFiles[testIndex]), 'datasetY_{}.csv'.format(listOfFiles[testIndex]))\n",
    "    predY=deepModel.predict_on_batch(dataX)\n",
    "    true_positives, predicted_positives, possible_positives = countPredictions(dataY, predY)\n",
    "    counts[deepModel.name]['true_positives'] += true_positives\n",
    "    counts[deepModel.name]['predicted_positives'] += predicted_positives\n",
    "    counts[deepModel.name]['possible_positives'] += possible_positives\n",
    "    print ('Testing deep model {} / {} : - true +ve:{}  pred +ve:{} possible +ve:{}'.format(testIndex+1, numberOfFiles, true_positives,predicted_positives,possible_positives), end='\\r')\n",
    "\n",
    "#Metrics reporting\n",
    "count = counts[deepModel.name]\n",
    "precision = (count['true_positives'])/(count['predicted_positives']+0.0001)\n",
    "recall = (count['true_positives'])/(count['possible_positives']+0.0001)\n",
    "f1score = 2.0 * precision * recall / (precision+recall+0.0001)\n",
    "print(' - Final Test Score for {} \\t f1-score = {:.4f}\\t precision = {:.4f} \\t recall = {:.4f}'.format(deepModel.name, f1score, precision, recall))\n",
    "\n",
    "#Save the model\n",
    "deepModel.save('deepModelDBpediaOntologyTypes.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. Eon 1/8\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9112\t precision = 0.9350 \t recall = 0.8886\n",
    "2. Eon 2/8\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9174\t precision = 0.9359 \t recall = 0.8997\n",
    "3. Eon 3/8\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9195\t precision = 0.9365 \t recall = 0.9032\n",
    "4. Eon 4/8\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9194\t precision = 0.9340 \t recall = 0.9054\n",
    "5. Eon 5/8\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9205\t precision = 0.9368 \t recall = 0.9048\n",
    "6. Eon 6/8\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9202\t precision = 0.9359 \t recall = 0.9051\n",
    "7. Eon 7/8\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9203\t precision = 0.9351 \t recall = 0.9061\n",
    "8. Eon 8/8\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.9199\t precision = 0.9350 \t recall = 0.9053\n",
    " - Final Test Score for Deep Model (5 Dense Layers) \t f1-score = 0.9200\t precision = 0.9352 \t recall = 0.9054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
